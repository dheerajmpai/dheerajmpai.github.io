<!DOCTYPE html><html>
<head>
    
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>SpeechPerfect: Collaboration of Multiple Modules for Perfect Speech Synthesis</title>
<!--Generated on Thu Nov 23 20:54:54 2023 by LaTeXML (version 0.8.6) http://dlmf.nist.gov/LaTeXML/.-->
<!-- 
<link rel="stylesheet" href="LaTeXML.css" type="text/css">
<link rel="stylesheet" href="ltx-article.css" type="text/css">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> -->


<link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
rel="stylesheet">

<link rel="stylesheet" href="./static/css/bulma.min.css">
<link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
<link rel="stylesheet" href="./static/css/bulma-slider.min.css">
<link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
<link rel="stylesheet"
href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
<link rel="stylesheet" href="./static/css/index.css">
<link rel="icon" href="./static/images/favicon.svg">

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script defer src="./static/js/fontawesome.all.min.js"></script>
<script src="./static/js/bulma-carousel.min.js"></script>
<script src="./static/js/bulma-slider.min.js"></script>
<script src="./static/js/index.js"></script>

</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">SpeechPerfect: Collaboration of Multiple Modules for Perfect Speech Synthesis</h1>
<section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Authors Information</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                Jeihee Cho<br>
                Carnegie Mellon University<br>
                Pittsburgh, PA 15213<br>
                <a href="mailto:jeiheec@andrew.cmu.edu">jeiheec@andrew.cmu.edu</a>
              </span>
              <span class="author-block">
                Moukhik Misra<br>
                Carnegie Mellon University<br>
                Pittsburgh, PA 15213<br>
                <a href="mailto:moukhikm@andrew.cmu.edu">moukhikm@andrew.cmu.edu</a>
              </span>
              <span class="author-block">
                Aarya Makwana<br>
                Carnegie Mellon University<br>
                Pittsburgh, PA 15217<br>
                <a href="mailto:amakwana@andrew.cmu.edu">amakwana@andrew.cmu.edu</a>
              </span>
              <span class="author-block">
                Tong Jiao<br>
                Carnegie Mellon University<br>
                Pittsburgh, PA 15213<br>
                <a href="mailto:tongjiao@andrew.cmu.edu">tongjiao@andrew.cmu.edu</a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">Identical speech reconstruction is a scarcely explored problem in the field of speech processing. Speech recognition and speech synthesis are currently more popular avenues of research. Our motivation is that if a model can reconstruct an identical speech only with features extracted from it, these features are shown to be effective in representing this speech signal. In this paper, we derive and improve upon the state-of-the-art in both speech recognition and speech synthesis to present <em class="ltx_emph ltx_font_italic">SpeechPerfect</em>, an end-to-end speech reconstruction model that fundamentally fuses Automatic Speech Recognition (ASR) and speech synthesis to regenerate the input speech signal. On a high level, the system utilizes phoneme embeddings that are generated by state-of-the-art ASR, and speaker characteristics which are obtained by our proposed speaker embedding extractor module to reconstruct input speech. Our model leverages an encoder-decoder architecture with a variance adapter module. The variance adaptor module predicts the pitch, energy, and duration of input speech and in combination with the speaker embedding is capable of replicating the target speech given only a few seconds of inputs. Notably, this paper proposes a complete, automatic end-to-end speech recognition system, capable of replicating input speech without relying on human efforts.
</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p">Speech recognition and synthesis are classical problems in the field of signal processing and computational linguistics. Speech recognition involves converting input speech to text and speech synthesis involves converting text to target speech. The complexities of speech recognition lie in dealing with the varied characteristics of human speech such as dealing with accents, speech pitch, quality, tonality, intonations and such. Traditional machine learning approaches to speech recognition have included Hidden Markov Model (HMM) <cite class="ltx_cite ltx_citemacro_citep">(Gales and Young, <a href="#bib.bib8" title="The application of hidden markov models in speech recognition" class="ltx_ref">2007</a>)</cite> and Dynamic Time Warping <cite class="ltx_cite ltx_citemacro_citep">(Amin and Mahmood, <a href="#bib.bib9" title="Speech recognition using dynamic time warping" class="ltx_ref">2008</a>)</cite>. More recent approaches to Speech Recognition have involved using CNN’s, RNN’s and Attention and Transformer models. Newer approaches have even considered End-to-End Speech Recognition that removes the need for acoustic, pronunciation and language models and directly converts speech to text. Some of these advances in Speech Recognition are models such as <em class="ltx_emph ltx_font_italic">wav2vec</em> <cite class="ltx_cite ltx_citemacro_citep">(Schneider<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib1" title="Wav2vec: unsupervised pre-training for speech recognition" class="ltx_ref">2019</a>)</cite> and its extension <em class="ltx_emph ltx_font_italic">wav2vec 2.0</em> <cite class="ltx_cite ltx_citemacro_citep">(Baevski<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib10" title="Wav2vec 2.0: a framework for self-supervised learning of speech representations" class="ltx_ref">2020</a>)</cite> and Conformer based models for Automatic Speech Recognition (ASR) <cite class="ltx_cite ltx_citemacro_citep">(Gulati<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib11" title="Conformer: convolution-augmented transformer for speech recognition" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p">Speech synthesis on the other hand refers to the process of generating speech from text with the need for understanding context in text and grammar and converting the same to speech signals. Traditional speech synthesis is a complex and multi-step process with steps such as sound wave generation and phonetic analysis. Traditional approaches to speech synthesis include Parametric Speech Synthesis using HMMs <cite class="ltx_cite ltx_citemacro_citep">(Zen, <a href="#bib.bib12" title="Statistical parametric speech synthesis: from hmm to lstm-rnn" class="ltx_ref">2015</a>)</cite> and Concatenative Speech Synthesis <cite class="ltx_cite ltx_citemacro_citep">(Longster, <a href="#bib.bib13" title="Concatenative speech synthesis : a framework for reducing perceived distortion when using the td-psola algorithm" class="ltx_ref">2003</a>)</cite>. Recent state-of-the-art approaches to speech synthesis are, however, all rooted in deep learning. Some of the best speech synthesis methods currently are <em class="ltx_emph ltx_font_italic">Tacotron2</em> <cite class="ltx_cite ltx_citemacro_citep">(Shen<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib14" title="Natural tts synthesis by conditioning wavenet on mel spectrogram predictions" class="ltx_ref">2018</a>)</cite> and <em class="ltx_emph ltx_font_italic">FastSpeech2</em> <cite class="ltx_cite ltx_citemacro_citep">(Ren<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib3" title="FastSpeech 2: fast and high-quality end-to-end text to speech" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p">With the objective of simplifying language problem to simple text-to-text problems, <cite class="ltx_cite ltx_citemacro_citep">(Raffel<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib15" title="Exploring the limits of transfer learning with a unified text-to-text transformer" class="ltx_ref">2023</a>)</cite> came up with the T5 framework which stands for Text-To-Text Transfer Transformer. Adapting this to speech tasks, <cite class="ltx_cite ltx_citemacro_citep">(Ao<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib2" title="SpeechT5: unified-modal encoder-decoder pre-training for spoken language processing" class="ltx_ref">2022</a>)</cite> developed <em class="ltx_emph ltx_font_italic">SpeechT5</em>, consisting of a shared encoder-decoder network capable of voice conversion, ASR, Text to Speech (TTS), and more. However, there exists a flaw in <em class="ltx_emph ltx_font_italic">SpeechT5</em>, it requires the use of external speaker embeddings for the purposes of TTS which makes it difficult for reconstruction of original speech.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p">To the best of our knowledge, not too many attempts have been made to improve end-to-end speech reconstruction i.e., replication of target input speech. To pave the way for advancements in the field, we aim to reconstruct the input speech and replicate it perfectly in a complete end-to-end system. We achieve this by using a multi-step process that utilizes an encoder-decoder network with the fundamental architecture closely resembling that of a fused ASR and TTS system.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p class="ltx_p">The main contribution can summarized as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p class="ltx_p">We introduce an end-to-end speech reconstruction module that requires no other inputs other than speech.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p class="ltx_p">We leverage the effective speaker characteristics extractor module with an encoder-decoder network and variance adaptor for enhance the reconstruction performance.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p class="ltx_p">By combining self-supervised speech encoding, non-autoregressive parallel synthesis, and speaker adaptation, our objective is to attain complete end-to-end speech reconstruction.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p class="ltx_p">We will conduct extensive experiments using VCTK dataset which consists of multi-accented utterances of 109 native English speakers.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p">Enhancing speech synthesis performance dragged the attention of the researchers as it can be applied to diverse downstream tasks such as speech enhancement, voice conversion, and more. The foundation of our paper is grounded in an investigation of papers related to ASR and TTS.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Automatic Speech Recognition</span>
Traditionally, researchers focus on generating speech representations for an automatic transcript or phoneme generation. The authors of <em class="ltx_emph ltx_font_italic">wav2vec</em> <cite class="ltx_cite ltx_citemacro_citep">(Schneider<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib1" title="Wav2vec: unsupervised pre-training for speech recognition" class="ltx_ref">2019</a>)</cite> suggested an unsupervised speech representation learning method using a contrastive predictive coding objective. It uses a convolutional neural network that takes raw audio as input and computes generate representation to give input as a speech recognition module. Moreover, building upon this foundation, <em class="ltx_emph ltx_font_italic">wav2vec 2.0</em> <cite class="ltx_cite ltx_citemacro_citep">(Baevski<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib10" title="Wav2vec 2.0: a framework for self-supervised learning of speech representations" class="ltx_ref">2020</a>)</cite> and <em class="ltx_emph ltx_font_italic">ReVISE</em> <cite class="ltx_cite ltx_citemacro_citep">(Hsu<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib16" title="ReVISE: self-supervised speech resynthesis with visual input for universal and generalized speech enhancement" class="ltx_ref">2022</a>)</cite> learn powerful speech encodings from raw audio in a completely unsupervised manner. Specifically, <em class="ltx_emph ltx_font_italic">ReVISE</em> contains a shared speech encoder, separate audio and visual decoders, and a contrastive loss between the two modality decoders to align the learned representations.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Text-to-Speech</span>
To generate realistic speech that goes along with the given text, researchers focus on producing diverse styles of speech by predicting the characteristics of speech. In <em class="ltx_emph ltx_font_italic">FastSpeech</em> <cite class="ltx_cite ltx_citemacro_citep">(Ren<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib4" title="FastSpeech: fast, robust and controllable text to speech" class="ltx_ref">2019</a>)</cite> and <em class="ltx_emph ltx_font_italic">FastSpeech2</em> <cite class="ltx_cite ltx_citemacro_citep">(Ren<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib3" title="FastSpeech 2: fast and high-quality end-to-end text to speech" class="ltx_ref">2022</a>)</cite>, they employ a variance predictor that predicts energy, pitch, and duration, which is trained by comparing the ground truth values with mean square error (MSE) loss. With predictor coupled with the phoneme embeddings, the researchers proposed a successful framework that generates speech with the given transcript.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p class="ltx_p"><em class="ltx_emph ltx_font_bold ltx_font_italic">SpeechT5</em>
For ultimate general speech <em class="ltx_emph ltx_font_italic">SpeechT5</em> <cite class="ltx_cite ltx_citemacro_citep">(Ao<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib2" title="SpeechT5: unified-modal encoder-decoder pre-training for spoken language processing" class="ltx_ref">2022</a>)</cite>, a unified encoder-decoder model pre-trained on large amounts of unlabeled speech and text data. <em class="ltx_emph ltx_font_italic">SpeechT5</em> converts spoken language tasks like ASR, TTS, and speech translation into a speech-to-speech framework. It uses a shared encoder-decoder backbone and modal-specific pre/post-nets. The model is pre-trained using a denoising sequence-to-sequence method and a novel cross-modal vector quantization approach to align textual and acoustic representations. However, this work failed to provide a framework for the reconstruction of speech as they focused on extracting general speech or text representation for downstream tasks such as TTS or ASR. Furthermore, an examination of the influence of speaker embedding on the vocoder was not undertaken, thereby resulting in a lack of comprehensive integration within the entirety of the end-to-end system for speech generation.
</p>
</div>
<div id="S2.p5" class="ltx_para">
<p class="ltx_p">The aforementioned studies require additional components or datasets besides the input speech to reconstruct the target speech. To tackle this challenge, we introduce <em class="ltx_emph ltx_font_italic">SpeechPerfect</em>, an advanced end-to-end speech reconstruction framework designed to account for the inherent characteristics of both speech and transcript automatically. This is achieved through the integration of innovative ASR and TTS modules within the proposed framework.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p class="ltx_p">This paper is organized as follows: After describing the baseline in Sec. <a href="#S3" title="3 Baseline ‣ SpeechPerfect: Collaboration of Multiple Modules for Perfect Speech Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we present our system model in Sec. <a href="#S4" title="4 SpeechPerfect ‣ SpeechPerfect: Collaboration of Multiple Modules for Perfect Speech Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Baseline</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p">We introduce two distinct baselines within our framework: <em class="ltx_emph ltx_font_italic">SpeechT5</em> and a tailored baseline amalgamating ASR and TTS. The <em class="ltx_emph ltx_font_italic">SpeechT5</em> baseline is designed with a primary focus on employing a pre-trained model versatile enough for diverse downstream tasks. Specifically, a fine-tuned model for voice conversion is derived from <em class="ltx_emph ltx_font_italic">SpeechT5</em>, presenting an adaptable foundation for use as a baseline.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p class="ltx_p">Conversely, the naive baseline integrates <em class="ltx_emph ltx_font_italic">wav2vec</em> and <em class="ltx_emph ltx_font_italic">FastSpeech2</em> in a combined approach for the speech-to-speech transformation. This integration enables us to assess the rudimentary performance of the suggested framework. By establishing these two baselines, we aim to facilitate a comprehensive comparative analysis, thereby elucidating the efficacy of our proposed framework in relation to established methodologies.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Ground truth and predicted transcript by <em class="ltx_emph ltx_font_italic">wav2vec</em> where wrong prediction is marked as blue</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text" style="font-size:90%;">Ground Truth</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text" style="font-size:90%;">Predicted</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_t" style="width:184.9pt;">
<p class="ltx_p ltx_align_top"><span class="ltx_text" style="font-size:90%;">MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL</span></p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" style="width:184.9pt;">
<p class="ltx_p ltx_align_top"><span class="ltx_text" style="font-size:90%;">MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL</span></p>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_t" style="width:184.9pt;">
<p class="ltx_p ltx_align_top"><span class="ltx_text" style="font-size:90%;">NOR IS MISTER QUILTER’S MANNER LESS INTERESTING THAN HIS MATTER</span></p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" style="width:184.9pt;">
<p class="ltx_p ltx_align_top"><span class="ltx_text" style="font-size:90%;">NOR IS MISTER QUILTER’S MANNER LESS INTERESTING THAN HIS MATTER</span></p>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_t" style="width:184.9pt;">
<p class="ltx_p ltx_align_top"><span class="ltx_text" style="font-size:90%;">HE TELLS US THAT AT THIS FESTIVE SEASON OF THE YEAR WITH </span><span class="ltx_text" style="font-size:90%;color:#0000FF;">CHRISTMAS</span><span class="ltx_text" style="font-size:90%;"> AND ROAST BEEF LOOMING BEFORE US SIMILES DRAWN FROM EATING AND ITS RESULTS OCCUR MOST READILY TO THE MIND</span></p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" style="width:184.9pt;">
<p class="ltx_p ltx_align_top"><span class="ltx_text" style="font-size:90%;">HE TELLS US THAT AT THIS FESTIVE SEASON OF THE YEAR WITH </span><span class="ltx_text" style="font-size:90%;color:#0000FF;">CHRISTMANUS</span><span class="ltx_text" style="font-size:90%;"> AND ROAST BEEF LOOMING BEFORE US SIMILES DRAWN FROM EATING AND ITS RESULTS OCCUR MOST READILY TO THE MIND</span></p>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" style="width:184.9pt;">
<p class="ltx_p ltx_align_top"><span class="ltx_text" style="font-size:90%;color:#0000FF;">LINNELL’S</span><span class="ltx_text" style="font-size:90%;"> PICTURES ARE A SORT OF UP GUARDS AND </span><span class="ltx_text" style="font-size:90%;color:#0000FF;">AT EM</span><span class="ltx_text" style="font-size:90%;"> PAINTINGS AND MASON’S EXQUISITE </span><span class="ltx_text" style="font-size:90%;color:#0000FF;">IDYLLS</span><span class="ltx_text" style="font-size:90%;"> ARE AS NATIONAL AS A JINGO POEM MISTER </span><span class="ltx_text" style="font-size:90%;color:#0000FF;">BIRKET FOSTER’S</span><span class="ltx_text" style="font-size:90%;"> LANDSCAPES SMILE AT ONE MUCH IN THE SAME WAY THAT MISTER CARKER USED TO FLASH HIS TEETH AND MISTER JOHN </span><span class="ltx_text" style="font-size:90%;color:#0000FF;">COLLIER</span><span class="ltx_text" style="font-size:90%;"> GIVES HIS SITTER A CHEERFUL SLAP ON THE BACK BEFORE HE SAYS LIKE A </span><span class="ltx_text" style="font-size:90%;color:#0000FF;">SHAMPOOER</span><span class="ltx_text" style="font-size:90%;"> IN A TURKISH BATH NEXT MAN</span></p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" style="width:184.9pt;">
<p class="ltx_p ltx_align_top"><span class="ltx_text" style="font-size:90%;color:#0000FF;">LINILL’S</span><span class="ltx_text" style="font-size:90%;"> PICTURES ARE A SORT OF UP GUARDS AND </span><span class="ltx_text" style="font-size:90%;color:#0000FF;">P</span><span class="ltx_text" style="font-size:90%;">AINTINGS AND MASON’S EXQUISITE </span><span class="ltx_text" style="font-size:90%;color:#0000FF;">ITLS</span><span class="ltx_text" style="font-size:90%;"> ARE AS NATIONAL AS A JINGO POEM MISTER </span><span class="ltx_text" style="font-size:90%;color:#0000FF;">BIRKATT FOSTERS</span><span class="ltx_text" style="font-size:90%;"> LANDSCAPES SMILE AT ONE MUCH IN THE SAME WAY THAT MISTER CARKER USED TO FLASH HIS TEETH AND MISTER JOHN </span><span class="ltx_text" style="font-size:90%;color:#0000FF;">CAWLEYER</span><span class="ltx_text" style="font-size:90%;"> GIVES HIS SITTER A CHEERFUL SLAP ON THE BACK BEFORE HE SAYS LIKE A </span><span class="ltx_text" style="font-size:90%;color:#0000FF;">CHAMPOOLER</span><span class="ltx_text" style="font-size:90%;"> IN A TURKISH BATH NEXT MAN</span></p>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.p3" class="ltx_para">
<p class="ltx_p"><em class="ltx_emph ltx_font_bold ltx_font_italic">SpeechT5-mod</em>
We were able to obtain a baseline of our paper by modifying the <em class="ltx_emph ltx_font_italic">SpeechT5</em>, namely <em class="ltx_emph ltx_font_italic">SpeechT5-mod</em>. As delineated in Section <a href="#S2" title="2 Related Work ‣ SpeechPerfect: Collaboration of Multiple Modules for Perfect Speech Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the versatility of <em class="ltx_emph ltx_font_italic">SpeechT5</em> extends to various downstream tasks, including its applicability within a speech-to-speech framework. In this context, we extract speaker embeddings utilizing the TDNN model provided by Hugging Face <cite class="ltx_cite ltx_citemacro_citep">(Ravanelli<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib7" title="SpeechBrain: a general-purpose speech toolkit" class="ltx_ref">2021</a>)</cite>. These embeddings serve as input to the vocoder, thereby facilitating the generation of speech that faithfully reproduces the characteristics of the input speech. Preliminary results of <em class="ltx_emph ltx_font_italic">SpeechT5-mod</em>, devoid of fine-tuning, are accessible through the following link: <a href="https://drive.google.com/drive/folders/1hrISi2Yzqfs8JtghvkDCxty-eYhDMVUe?usp=drive_link" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://drive.google.com/drive/folders/1hrISi2Yzqfs8JtghvkDCxty-eYhDMVUe?usp=drive_link</a>.
</p>
</div>
<div id="S3.p4" class="ltx_para">
<p class="ltx_p"><em class="ltx_emph ltx_font_bold ltx_font_italic">Custom Baseline</em> In our preliminary baseline assessment, we leverage a segment of our module to gauge performance. Employing <em class="ltx_emph ltx_font_italic">wav2vec</em> for Automatic Speech Recognition (ASR) proves advantageous, given its widespread utilization and commendable proficiency in extracting phonetic elements or transcriptions from speech. A tangible illustration of the predicted transcript corresponding to a provided speech sample is presented in Table <a href="#S3.T1" title="Table 1 ‣ 3 Baseline ‣ SpeechPerfect: Collaboration of Multiple Modules for Perfect Speech Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p class="ltx_p">In the implementation of the TTS module, we employ the <em class="ltx_emph ltx_font_italic">FastSpeech2</em> algorithm, which operates by synthesizing speech based on the provided transcript through the prediction of duration, speech, and energy parameters, all without reliance on speaker-specific information. To initiate the speech synthesis process with <em class="ltx_emph ltx_font_italic">FastSpeech2</em>, an initial speech sample is utilized as input to the <em class="ltx_emph ltx_font_italic">wav2vec</em> module. Subsequently, the output of the <em class="ltx_emph ltx_font_italic">wav2vec</em> module serves as the input to <em class="ltx_emph ltx_font_italic">FastSpeech2</em>, facilitating the generation of speech that faithfully reproduces the original input transcript. The resultant example waveform file, generated through this foundational approach, can be accessed via the following hyperlink:
<a href="https://drive.google.com/drive/folders/15U0o9rIJEOxMDgylWGzcYPtM8ukKLfET?usp=drive_link" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://drive.google.com/drive/folders/15U0o9rIJEOxMDgylWGzcYPtM8ukKLfET?usp=drive_link</a>.</p>
</div>
<div id="S3.p6" class="ltx_para">
<p class="ltx_p">The implementation of the baseline can be found from the link <a href="https://drive.google.com/file/d/12jqCBBbc07jjS9BUVeg21iYg1cfpBc0s/view?usp=sharing" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://drive.google.com/file/d/12jqCBBbc07jjS9BUVeg21iYg1cfpBc0s/view?usp=sharing</a>.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>SpeechPerfect</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p">On top of the illustrated baseline, we added a component to provide information about the speaker to the decoder to generate speech that is identical to the given speech. The existing baselines normally focus on either generating speech with given text by predicting characteristics of speech or modifying the style of the speech, which eventually fails to generate speech identical to the given input. Therefore, by extracting additional features from the speech, we aim to reproduce the entered speech.
</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Model Overview</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p class="ltx_p">The overall architecture of our model is shown in Figure 1. Our model is an improvement of FastSpeech2. The model accepts a piece of speech waveform as an input and aims to reconstruct an identical waveform as an output. First, the phoneme embedding layer takes in a phoneme sequence, which is extracted from the original input waveform by the Automatic Speech Recognition (ASR) module. The phoneme sequence is then converted to a phoneme hidden sequence by the encoder, after applying positional encoding to it. Next, the variance adaptor adds different speaker-specific information to the phoneme hidden sequence, such as speaker embedding (from the characteristics extractor), duration, pitch, and energy. Finally, the decoder converts the hidden sequence with added information into a mel-spectrogram sequence, and HiFi-GAN module transfers that into the reconstructed output.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Model components description</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">ASR</span> <em class="ltx_emph ltx_font_italic">wav2vec</em> <cite class="ltx_cite ltx_citemacro_citep">(Schneider<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib1" title="Wav2vec: unsupervised pre-training for speech recognition" class="ltx_ref">2019</a>)</cite> helps us to get a phoneme sequence <math id="S4.SS2.p1.m1" class="ltx_Math" alttext="(x_{1},\dots,x_{n})" display="inline"><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="false">)</mo></mrow></math> from the original input.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Phoneme Embedding</span> This module converts a sequence of phonemes <math id="S4.SS2.p2.m1" class="ltx_Math" alttext="(x_{1},\dots,x_{n})" display="inline"><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="false">)</mo></mrow></math> into a sequence of vectors with dimension <math id="S4.SS2.p2.m2" class="ltx_Math" alttext="d=256" display="inline"><mrow><mi>d</mi><mo>=</mo><mn>256</mn></mrow></math> and then applies positional encoding to this sequence to get a phoneme hidden sequence <math id="S4.SS2.p2.m3" class="ltx_Math" alttext="(y_{1},\dots,y_{n})" display="inline"><mrow><mo stretchy="false">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>y</mi><mi>n</mi></msub><mo stretchy="false">)</mo></mrow></math>, with <math id="S4.SS2.p2.m4" class="ltx_Math" alttext="y_{i}\in\mathbb{R}^{d}" display="inline"><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow></math>. The positional encoding used is the same as that used by Transformer <cite class="ltx_cite ltx_citemacro_citep">(Vaswani<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib5" title="Attention is all you need" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Encoder and decoder</span> Encoder and decoder architecture in our model is similar to that in <em class="ltx_emph ltx_font_italic">FastSpeech2</em> <cite class="ltx_cite ltx_citemacro_citep">(Ren<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib3" title="FastSpeech 2: fast and high-quality end-to-end text to speech" class="ltx_ref">2022</a>)</cite>, they comprise a stack of feed-forward Transformer blocks and a 1D-convolution layer. After the encoder module, the sequence length is unchanged and each element in the sequence has <math id="S4.SS2.p3.m1" class="ltx_Math" alttext="d=256" display="inline"><mrow><mi>d</mi><mo>=</mo><mn>256</mn></mrow></math> dimensions. The decoder module converts the input sequence to a mel spectrogram.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Characteristics Extractor</span> To produce a piece of speech that is as similar as possible to the input, the model needs to capture some information that is specific to the input. This module and the Variance Adaptor module aim to capture this kind of information. This module focuses on information that is not specific to each phoneme in the input sequence, while the Variance Adaptor calculates features both input-specific and phoneme-specific and aggregates them with results from this module. Now this module only calculates a 256-d speaker embedding and passes it to Variance Adaptor for future use. Information besides speaker embedding can be added depending on our future experiment results. To get a speaker embedding, we use a trained model based on GE2E loss provided by <cite class="ltx_cite ltx_citemacro_citep">(Wan<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib6" title="Generalized end-to-end loss for speaker verification" class="ltx_ref">2020</a>)</cite>. This model calculates a speaker embedding from 16kHz audio input, so we pass the original input directly to this model.</p>
</div>
<figure id="S4.F1" class="ltx_figure">
<table style="width:100%;">
<tr>
<td class="ltx_subfigure">
<figure id="S4.F1.sf1" class="ltx_figure ltx_align_center"><img src="arch_1.png" id="S4.F1.sf1.g1" class="ltx_graphics" alt="The overall architecture of ">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:80%;">(a)</span> </span><em class="ltx_emph ltx_font_italic" style="font-size:80%;">SpeechPerfect</em></figcaption>
</figure>
</td>
<td class="ltx_subfigure">
<figure id="S4.F1.sf2" class="ltx_figure ltx_align_center"><img src="arch_2.png" id="S4.F1.sf2.g1" class="ltx_graphics" alt="The overall architecture of ">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:80%;">(b)</span> </span><span class="ltx_text" style="font-size:80%;">Variance Adaptor</span></figcaption>
</figure>
</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The overall architecture of <em class="ltx_emph ltx_font_italic">SpeechPerfect</em>. Subfigure (b) expands the structure of the variance adaptor.</figcaption>
</figure>
<div id="S4.SS2.p5" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Variance Adaptor</span> The goal of the variance adaptor is to aggregate the characteristics that are specific to a certain piece of speech, including pitch, energy, duration, and speaker-specific information. By adding this information to the phoneme hidden sequence, the decoder is capable of generating a representation that is specific to a certain input and making it as similar to the input as possible. The original variance adaptor in our baseline contains three predictors capturing three kinds of information: duration, pitch, and energy for each phoneme <math id="S4.SS2.p5.m1" class="ltx_Math" alttext="z_{i}" display="inline"><msub><mi>z</mi><mi>i</mi></msub></math> in the phoneme hidden sequence. Each predictor is made up of two 1D-convolution layers and a linear layer.</p>
</div>
<div id="S4.SS2.p6" class="ltx_para">
<p class="ltx_p">The input to duration predictor is the phoneme hidden sequence <math id="S4.SS2.p6.m1" class="ltx_Math" alttext="(y_{1},\dots,y_{n})" display="inline"><mrow><mo stretchy="false">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>y</mi><mi>n</mi></msub><mo stretchy="false">)</mo></mrow></math>, and the duration predictor predicts how many mel frames correspond to each frame. After knowing that, we expand the phoneme hidden sequence to a sequence <math id="S4.SS2.p6.m2" class="ltx_Math" alttext="(z_{1},\dots,z_{m})" display="inline"><mrow><mo stretchy="false">(</mo><msub><mi>z</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>z</mi><mi>m</mi></msub><mo stretchy="false">)</mo></mrow></math> with a length <math id="S4.SS2.p6.m3" class="ltx_Math" alttext="m" display="inline"><mi>m</mi></math> equal to the number of frames of the input to prepare for the next two predictors. The pitch predictor and energy predictor take the expanded sequence as input, and predict 1) pitch <math id="S4.SS2.p6.m4" class="ltx_Math" alttext="F_{0}" display="inline"><msub><mi>F</mi><mn>0</mn></msub></math> for each frame, and 2) L2-norm of the amplitude of each Short-Time Fourier Transform (STFT) frame correspondingly. Predicted results are embedded into a <math id="S4.SS2.p6.m5" class="ltx_Math" alttext="d=256" display="inline"><mrow><mi>d</mi><mo>=</mo><mn>256</mn></mrow></math> dimension vector and added to corresponding <math id="S4.SS2.p6.m6" class="ltx_Math" alttext="z_{i}" display="inline"><msub><mi>z</mi><mi>i</mi></msub></math> in the expanded sequence. The result from the characteristics extractor is added to every <math id="S4.SS2.p6.m7" class="ltx_Math" alttext="z_{i}" display="inline"><msub><mi>z</mi><mi>i</mi></msub></math>. In sum, each element in the expanded sequence is calculated by</p>
<table id="S4.Ex1" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.Ex1.m1" class="ltx_Math" alttext="z_{i}=y_{phoneme}+p_{i}+e_{i}+s" display="block"><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>=</mo><mrow><msub><mi>y</mi><mrow><mi>p</mi><mo>⁢</mo><mi>h</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><mi>n</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mi>m</mi><mo>⁢</mo><mi>e</mi></mrow></msub><mo>+</mo><msub><mi>p</mi><mi>i</mi></msub><mo>+</mo><msub><mi>e</mi><mi>i</mi></msub><mo>+</mo><mi>s</mi></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">Where <math id="S4.SS2.p6.m8" class="ltx_Math" alttext="y_{phoneme}" display="inline"><msub><mi>y</mi><mrow><mi>p</mi><mo>⁢</mo><mi>h</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><mi>n</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mi>m</mi><mo>⁢</mo><mi>e</mi></mrow></msub></math> is the corresponding phoneme of <math id="S4.SS2.p6.m9" class="ltx_Math" alttext="z_{i}" display="inline"><msub><mi>z</mi><mi>i</mi></msub></math> in the phoneme hidden sequence, <math id="S4.SS2.p6.m10" class="ltx_Math" alttext="p_{i}" display="inline"><msub><mi>p</mi><mi>i</mi></msub></math> is the result from pitch predictor for the <math id="S4.SS2.p6.m11" class="ltx_Math" alttext="i^{th}" display="inline"><msup><mi>i</mi><mrow><mi>t</mi><mo>⁢</mo><mi>h</mi></mrow></msup></math> frame, <math id="S4.SS2.p6.m12" class="ltx_Math" alttext="e_{i}" display="inline"><msub><mi>e</mi><mi>i</mi></msub></math> is the result from energy predictor for the <math id="S4.SS2.p6.m13" class="ltx_Math" alttext="i^{th}" display="inline"><msup><mi>i</mi><mrow><mi>t</mi><mo>⁢</mo><mi>h</mi></mrow></msup></math> frame and <math id="S4.SS2.p6.m14" class="ltx_Math" alttext="s" display="inline"><mi>s</mi></math> is speaker embedding from the characteristics extractor.</p>
</div>
<div id="S4.SS2.p7" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">HiFi-GAN</span> To convert the predicted mel spectrogram created by the decoder module into an actual speech waveform, we used a vocoder based on HiFi-GAN <cite class="ltx_cite ltx_citemacro_citep">(Ao<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib2" title="SpeechT5: unified-modal encoder-decoder pre-training for spoken language processing" class="ltx_ref">2022</a>)</cite>.

</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Evaluation Plan</h2>

<div id="S5.p1" class="ltx_para">
<p class="ltx_p">We will experiment using VCTK dataset <cite class="ltx_cite ltx_citemacro_citep">(Yamagishi<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib17" title="CSTR VCTK Corpus: english multi-speaker corpus for CSTR voice cloning toolkit (version 0.92)" class="ltx_ref">2019</a>)</cite>, speech data with diverse English accents. We are planning to measure the quality of speech using mean absolute error (MAE), PESQ <cite class="ltx_cite ltx_citemacro_citep">(Rix<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib18" title="Perceptual evaluation of speech quality (pesq)-a new method for speech quality assessment of telephone networks and codecs" class="ltx_ref">2001</a>)</cite> and VisQOL <cite class="ltx_cite ltx_citemacro_citep">(Hines<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib19" title="ViSQOL: an objective speech quality model" class="ltx_ref">2015</a>)</cite>.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul id="bib.L1" class="ltx_biblist">
<li id="bib.bib9" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">T. B. Amin and I. Mahmood (2008)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Speech recognition using dynamic time warping</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://api.semanticscholar.org/CorpusID:22123601" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ SpeechPerfect: Collaboration of Multiple Modules for Perfect Speech Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">J. Ao, R. Wang, L. Zhou, C. Wang, S. Ren, Y. Wu, S. Liu, T. Ko, Q. Li, Y. Zhang, Z. Wei, Y. Qian, J. Li, and F. Wei (2022)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">SpeechT5: unified-modal encoder-decoder pre-training for spoken language processing</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">2110.07205</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p3" title="1 Introduction ‣ SpeechPerfect: Collaboration of Multiple Modules for Perfect Speech Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S2.p4" title="2 Related Work ‣ SpeechPerfect: Collaboration of Multiple Modules for Perfect Speech Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a href="#S4.SS2.p7" title="4.2 Model components description ‣ 4 SpeechPerfect ‣ SpeechPerfect: Collaboration of Multiple Modules for Perfect Speech Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.2</span></a>.
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. Baevski, H. Zhou, A. Mohamed, and M. Auli (2020)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Wav2vec 2.0: a framework for self-supervised learning of speech representations</span>.
</span>
<span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume">abs/2006.11477</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://api.semanticscholar.org/CorpusID:219966759" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ SpeechPerfect: Collaboration of Multiple Modules for Perfect Speech Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S2.p2" title="2 Related Work ‣ SpeechPerfect: Collaboration of Multiple Modules for Perfect Speech Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">M. J. F. Gales and S. J. Young (2007)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The application of hidden markov models in speech recognition</span>.
</span>
<span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume">1</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://api.semanticscholar.org/CorpusID:51039442" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ SpeechPerfect: Collaboration of Multiple Modules for Perfect Speech Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. Gulati, J. Qin, C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu, and R. Pang (2020)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Conformer: convolution-augmented transformer for speech recognition</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">2005.08100</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ SpeechPerfect: Collaboration of Multiple Modules for Perfect Speech Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. Hines, J. Skoglund, A. Kokaram, and N. Harte (2015)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">ViSQOL: an objective speech quality model</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">EURASIP Journal on Audio, Speech, and Music Processing</span> <span class="ltx_text ltx_bib_volume">2015 (13)</span>, <span class="ltx_text ltx_bib_pages"> pp. 1–18</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.p1" title="5 Evaluation Plan ‣ SpeechPerfect: Collaboration of Multiple Modules for Perfect Speech Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5</span></a>.
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">W. Hsu, T. Remez, B. Shi, J. Donley, and Y. Adi (2022)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">ReVISE: self-supervised speech resynthesis with visual input for universal and generalized speech enhancement</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">2212.11377</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p2" title="2 Related Work ‣ SpeechPerfect: Collaboration of Multiple Modules for Perfect Speech Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">J. Longster (2003)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Concatenative speech synthesis : a framework for reducing perceived distortion when using the td-psola algorithm</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://api.semanticscholar.org/CorpusID:46223196" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ SpeechPerfect: Collaboration of Multiple Modules for Perfect Speech Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu (2023)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Exploring the limits of transfer learning with a unified text-to-text transformer</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">1910.10683</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p3" title="1 Introduction ‣ SpeechPerfect: Collaboration of Multiple Modules for Perfect Speech Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">M. Ravanelli, T. Parcollet, P. Plantinga, A. Rouhe, S. Cornell, L. Lugosch, C. Subakan, N. Dawalatabad, A. Heba, J. Zhong, J. Chou, S. Yeh, S. Fu, C. Liao, E. Rastorgueva, F. Grondin, W. Aris, H. Na, Y. Gao, R. D. Mori, and Y. Bengio (2021)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">SpeechBrain: a general-purpose speech toolkit</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">arXiv:2106.04624</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">2106.04624</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.p3" title="3 Baseline ‣ SpeechPerfect: Collaboration of Multiple Modules for Perfect Speech Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3</span></a>.
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">Y. Ren, C. Hu, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T. Liu (2022)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">FastSpeech 2: fast and high-quality end-to-end text to speech</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">2006.04558</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ SpeechPerfect: Collaboration of Multiple Modules for Perfect Speech Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S2.p3" title="2 Related Work ‣ SpeechPerfect: Collaboration of Multiple Modules for Perfect Speech Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a href="#S4.SS2.p3" title="4.2 Model components description ‣ 4 SpeechPerfect ‣ SpeechPerfect: Collaboration of Multiple Modules for Perfect Speech Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.2</span></a>.
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">Y. Ren, Y. Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T. Liu (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">FastSpeech: fast, robust and controllable text to speech</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">CoRR</span> <span class="ltx_text ltx_bib_volume">abs/1905.09263</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://arxiv.org/abs/1905.09263" title="" class="ltx_ref ltx_bib_external">Link</a>,
<span class="ltx_text ltx_bib_external">1905.09263</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p3" title="2 Related Work ‣ SpeechPerfect: Collaboration of Multiple Modules for Perfect Speech Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A.W. Rix, J.G. Beerends, M.P. Hollier, and A.P. Hekstra (2001)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Perceptual evaluation of speech quality (pesq)-a new method for speech quality assessment of telephone networks and codecs</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">2001 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No.01CH37221)</span>,
</span>
<span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume">2</span>, <span class="ltx_text ltx_bib_pages"> pp. 749–752 vol.2</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://dx.doi.org/10.1109/ICASSP.2001.941023" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.p1" title="5 Evaluation Plan ‣ SpeechPerfect: Collaboration of Multiple Modules for Perfect Speech Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5</span></a>.
</span>
</li>
<li id="bib.bib1" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">S. Schneider, A. Baevski, R. Collobert, and M. Auli (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Wav2vec: unsupervised pre-training for speech recognition</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">CoRR</span> <span class="ltx_text ltx_bib_volume">abs/1904.05862</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://arxiv.org/abs/1904.05862" title="" class="ltx_ref ltx_bib_external">Link</a>,
<span class="ltx_text ltx_bib_external">1904.05862</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ SpeechPerfect: Collaboration of Multiple Modules for Perfect Speech Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S2.p2" title="2 Related Work ‣ SpeechPerfect: Collaboration of Multiple Modules for Perfect Speech Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a href="#S4.SS2.p1" title="4.2 Model components description ‣ 4 SpeechPerfect ‣ SpeechPerfect: Collaboration of Multiple Modules for Perfect Speech Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.2</span></a>.
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">J. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z. Yang, Z. Chen, Y. Zhang, Y. Wang, R. Skerry-Ryan, R. A. Saurous, Y. Agiomyrgiannakis, and Y. Wu (2018)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Natural tts synthesis by conditioning wavenet on mel spectrogram predictions</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">1712.05884</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ SpeechPerfect: Collaboration of Multiple Modules for Perfect Speech Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin (2023)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Attention is all you need</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">1706.03762</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS2.p2" title="4.2 Model components description ‣ 4 SpeechPerfect ‣ SpeechPerfect: Collaboration of Multiple Modules for Perfect Speech Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.2</span></a>.
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">L. Wan, Q. Wang, A. Papir, and I. L. Moreno (2020)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Generalized end-to-end loss for speaker verification</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">1710.10467</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS2.p4" title="4.2 Model components description ‣ 4 SpeechPerfect ‣ SpeechPerfect: Collaboration of Multiple Modules for Perfect Speech Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.2</span></a>.
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">J. Yamagishi, C. Veaux, and K. MacDonald (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">CSTR VCTK Corpus: english multi-speaker corpus for CSTR voice cloning toolkit (version 0.92)</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">University of Edinburgh. The Centre for Speech Technology Research (CSTR)</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://dx.doi.org/10.7488/ds/2645" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.p1" title="5 Evaluation Plan ‣ SpeechPerfect: Collaboration of Multiple Modules for Perfect Speech Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5</span></a>.
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">H. Zen (2015)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Statistical parametric speech synthesis: from hmm to lstm-rnn</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://api.semanticscholar.org/CorpusID:36067578" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ SpeechPerfect: Collaboration of Multiple Modules for Perfect Speech Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Nov 23 20:54:54 2023 by <a href="http://dlmf.nist.gov/LaTeXML/">LaTeXML <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"></a>
</div></footer>
</div>
</body>
</html>
